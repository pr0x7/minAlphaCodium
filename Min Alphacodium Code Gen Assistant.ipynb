{"metadata":{"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":30698,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":5,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Code generation with flow\n\nAlphaCodium presented an approach for code generation that uses control flow.\n\nMain idea: construct an answer to a coding question iteratively. \n\n[AlphaCodium](https://github.com/Codium-ai/AlphaCodium) iteravely tests and improves an answer on public and AI-generated tests for a particular question. \n\nWe will implement some of these ideas from scratch using Langgraph:\n\n1. We start with a set of documentation specified by a user\n2. We use a long context LLM to ingest it, and answer a question based upon it \n3. We perform two unit tests: Check imports and code execution\n\n","metadata":{}},{"cell_type":"code","source":"! pip install -U langchain_community langchain-openai langchain-anthropic langchain langgraph bs4","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"\n","metadata":{}},{"cell_type":"code","source":"from bs4 import BeautifulSoup as Soup\nfrom langchain_community.document_loaders.recursive_url_loader import RecursiveUrlLoader\n\nurl = \"--Use any documentation of your choice--\"\nloader = RecursiveUrlLoader(\n    url=url, max_depth=20, extractor=lambda x: Soup(x, \"html.parser\").text\n)\ndocs = loader.load()\n\n# Sort the list based on the URLs and get the text\nd_sorted = sorted(docs, key=lambda x: x.metadata[\"source\"])\nd_reversed = list(reversed(d_sorted))\nconcatenated_content = \"\\n\\n\\n --- \\n\\n\\n\".join(\n    [doc.page_content for doc in d_reversed]\n)","metadata":{},"execution_count":1,"outputs":[]},{"cell_type":"markdown","source":"## LLMs\n\n### Code solution\n","metadata":{}},{"cell_type":"code","source":"from langchain_openai import ChatOpenAI\nfrom langchain_core.prompts import ChatPromptTemplate\nfrom langchain_core.pydantic_v1 import BaseModel, Field\n\n\n# Grader prompt\ncode_gen_prompt = ChatPromptTemplate.from_messages(\n    [\n        (\n            \"system\",\n            \"\"\"You are a coding assistant with expertise in --sepcify the expertise--. \\n \n    Here is a full set of --your documentaion-- documentation:  \\n ------- \\n  {context} \\n ------- \\n Answer the user \n    question based on the above provided documentation. Ensure any code you provide can be executed \\n \n    with all required imports and variables defined. Structure your answer with a description of the code solution. \\n\n    Then list the imports. And finally list the functioning code block. Here is the user question:\"\"\",\n        ),\n        (\"placeholder\", \"{messages}\"),\n    ]\n)\n\n\n# Data model\nclass code(BaseModel):\n    \"\"\"Code output\"\"\"\n\n    prefix: str = Field(description=\"Description of the problem and approach\")\n    imports: str = Field(description=\"Code block import statements\")\n    code: str = Field(description=\"Code block not including import statements\")\n    description = \"Schema for code solutions to questions about LCEL.\"\n\n\nexpt_llm = \"gpt-4-0125-preview\"\nllm = ChatOpenAI(temperature=0, model=expt_llm)\ncode_gen_chain = code_gen_prompt | llm.with_structured_output(code)\nquestion = \"How do I build a RAG chain in LCEL?\"\n# solution = code_gen_chain_oai.invoke({\"context\":concatenated_content,\"messages\":[(\"user\",question)]})","metadata":{},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"from langchain_anthropic import ChatAnthropic\nfrom langchain_core.prompts import ChatPromptTemplate\nfrom langchain_core.pydantic_v1 import BaseModel, Field\n\n### Anthropic\n\n# Prompt to enforce tool use\ncode_gen_prompt_claude = ChatPromptTemplate.from_messages(\n    [\n        (\n            \"system\",\n            \"\"\"You are a coding assistant with expertise in --sepcify the expertise--. \\n \n    Here is a full set of --your documentaion-- documentation:  \\n ------- \\n  {context} \\n ------- \\n Answer the user \n    question based on the above provided documentation. Ensure any code you provide can be executed \\n \n    with all required imports and variables defined. Structure your answer with a description of the code solution. \\n\n    Then list the imports. And finally list the functioning code block. Here is the user question:\"\"\",\n        ),\n        (\"placeholder\", \"{messages}\"),\n    ]\n)\n\n\n# Data model\nclass code(BaseModel):\n    \"\"\"Code output\"\"\"\n\n    prefix: str = Field(description=\"Description of the problem and approach\")\n    imports: str = Field(description=\"Code block import statements\")\n    code: str = Field(description=\"Code block not including import statements\")\n    description = \"Schema for code solutions to questions about LCEL.\"\n\n\n# LLM\n# expt_llm = \"claude-3-haiku-20240307\"\nexpt_llm = \"claude-3-opus-20240229\"\nllm = ChatAnthropic(\n    model=expt_llm,\n    default_headers={\"anthropic-beta\": \"tools-2024-04-04\"},\n)\n\nstructured_llm_claude = llm.with_structured_output(code, include_raw=True)\n\n\n# Optional: Check for errors in case tool use is flaky\ndef check_claude_output(tool_output):\n    \"\"\"Check for parse error or failure to call the tool\"\"\"\n\n    # Error with parsing\n    if tool_output[\"parsing_error\"]:\n        # Report back output and parsing errors\n        print(\"Parsing error!\")\n        raw_output = str(code_output[\"raw\"].content)\n        error = tool_output[\"parsing_error\"]\n        raise ValueError(\n            f\"Error parsing your output! Be sure to invoke the tool. Output: {raw_output}. \\n Parse error: {error}\"\n        )\n\n    # Tool was not invoked\n    elif not tool_output[\"parsed\"]:\n        print(\"Failed to invoke tool!\")\n        raise ValueError(\n            f\"You did not use the provided tool! Be sure to invoke the tool to structure the output.\"\n        )\n    return tool_output\n\n\n# Chain with output check\ncode_chain_claude_raw = (\n    code_gen_prompt_claude | structured_llm_claude | check_claude_output\n)\n\n\ndef insert_errors(inputs):\n    \"\"\"Insert errors for tool parsing in the messages\"\"\"\n\n    # Get errors\n    error = inputs[\"error\"]\n    messages = inputs[\"messages\"]\n    messages += [\n        (\n            \"assistant\",\n            f\"Retry. You are required to fix the parsing errors: {error} \\n\\n You must invoke the provided tool.\",\n        )\n    ]\n    return {\n        \"messages\": messages,\n        \"context\": inputs[\"context\"],\n    }\n\n\n# This will be run as a fallback chain\nfallback_chain = insert_errors | code_chain_claude_raw\nN = 3  # Max re-tries\ncode_gen_chain_re_try = code_chain_claude_raw.with_fallbacks(\n    fallbacks=[fallback_chain] * N, exception_key=\"error\"\n)\n\n\ndef parse_output(solution):\n    \"\"\"When we add 'include_raw=True' to structured output,\n    it will return a dict w 'raw', 'parsed', 'parsing_error'.\"\"\"\n\n    return solution[\"parsed\"]\n\n\n# With re-try to correct for failure to invoke tool\n# TODO: Annoying errors w/ \"user\" vs \"assistant\"\n# Roles must alternate between \"user\" and \"assistant\", but found multiple \"user\" roles in a row\ncode_gen_chain = code_gen_chain_re_try | parse_output\n\n# No re-try\ncode_gen_chain = code_gen_prompt_claude | structured_llm_claude | parse_output","metadata":{},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"# Test\nquestion = \"PUT YOUR QUESTION ABOUT THE DOCUMENTATION\"\nsolution = code_gen_chain.invoke(\n    {\"context\": concatenated_content, \"messages\": [(\"user\", question)]}\n)\nsolution","metadata":{"scrolled":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## State \n\nOur state is a dict that will contain keys (errors, question, code generation) relevant to code generation.","metadata":{}},{"cell_type":"code","source":"from typing import Dict, TypedDict, List\n\n\nclass GraphState(TypedDict):\n    \"\"\"\n    Represents the state of our graph.\n\n    Attributes:\n        error : Binary flag for control flow to indicate whether test error was tripped\n        messages : With user question, error messages, reasoning\n        generation : Code solution\n        iterations : Number of tries\n    \"\"\"\n\n    error: str\n    messages: List\n    generation: str\n    iterations: int","metadata":{},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":"## Graph \n\nOur graph lays out the logical flow shown in the figure above.","metadata":{}},{"cell_type":"code","source":"from operator import itemgetter\nfrom langchain_core.pydantic_v1 import BaseModel, Field\nfrom langchain_core.runnables import RunnablePassthrough\nfrom langchain_core.prompts import PromptTemplate\n\n### Parameter\n\n# Max tries\nmax_iterations = 3\n# Reflect\n# flag = 'reflect'\nflag = \"do not reflect\"\n\n### Nodes\n\n\ndef generate(state: GraphState):\n    \"\"\"\n    Generate a code solution\n\n    Args:\n        state (dict): The current graph state\n\n    Returns:\n        state (dict): New key added to state, generation\n    \"\"\"\n\n    print(\"---GENERATING CODE SOLUTION---\")\n\n    # State\n    messages = state[\"messages\"]\n    iterations = state[\"iterations\"]\n    error = state[\"error\"]\n\n    # We have been routed back to generation with an error\n    if error == \"yes\":\n        messages += [\n            (\n                \"user\",\n                \"Now, try again. Invoke the code tool to structure the output with a prefix, imports, and code block:\",\n            )\n        ]\n\n    # Solution\n    code_solution = code_gen_chain.invoke(\n        {\"context\": concatenated_content, \"messages\": messages}\n    )\n    messages += [\n        (\n            \"assistant\",\n            f\"{code_solution.prefix} \\n Imports: {code_solution.imports} \\n Code: {code_solution.code}\",\n        )\n    ]\n\n    # Increment\n    iterations = iterations + 1\n    return {\"generation\": code_solution, \"messages\": messages, \"iterations\": iterations}\n\n\ndef code_check(state: GraphState):\n    \"\"\"\n    Check code\n\n    Args:\n        state (dict): The current graph state\n\n    Returns:\n        state (dict): New key added to state, error\n    \"\"\"\n\n    print(\"---CHECKING CODE---\")\n\n    # State\n    messages = state[\"messages\"]\n    code_solution = state[\"generation\"]\n    iterations = state[\"iterations\"]\n\n    # Get solution components\n    prefix = code_solution.prefix\n    imports = code_solution.imports\n    code = code_solution.code\n\n    # Check imports\n    try:\n        exec(imports)\n    except Exception as e:\n        print(\"---CODE IMPORT CHECK: FAILED---\")\n        error_message = [(\"user\", f\"Your solution failed the import test: {e}\")]\n        messages += error_message\n        return {\n            \"generation\": code_solution,\n            \"messages\": messages,\n            \"iterations\": iterations,\n            \"error\": \"yes\",\n        }\n\n    # Check execution\n    try:\n        exec(imports + \"\\n\" + code)\n    except Exception as e:\n        print(\"---CODE BLOCK CHECK: FAILED---\")\n        error_message = [(\"user\", f\"Your solution failed the code execution test: {e}\")]\n        messages += error_message\n        return {\n            \"generation\": code_solution,\n            \"messages\": messages,\n            \"iterations\": iterations,\n            \"error\": \"yes\",\n        }\n\n    # No errors\n    print(\"---NO CODE TEST FAILURES---\")\n    return {\n        \"generation\": code_solution,\n        \"messages\": messages,\n        \"iterations\": iterations,\n        \"error\": \"no\",\n    }\n\n\ndef reflect(state: GraphState):\n    \"\"\"\n    Reflect on errors\n\n    Args:\n        state (dict): The current graph state\n\n    Returns:\n        state (dict): New key added to state, generation\n    \"\"\"\n\n    print(\"---GENERATING CODE SOLUTION---\")\n\n    # State\n    messages = state[\"messages\"]\n    iterations = state[\"iterations\"]\n    code_solution = state[\"generation\"]\n\n    # Prompt reflection\n    reflection_message = [\n        (\n            \"user\",\n            \"\"\"You tried to solve this problem and failed a unit test. Reflect on this failure\n                                    given the provided documentation. Write a few key suggestions based on the \n                                    documentation to avoid making this mistake again.\"\"\",\n        )\n    ]\n\n    # Add reflection\n    reflections = code_gen_chain.invoke(\n        {\"context\": concatenated_content, \"messages\": messages}\n    )\n    messages += [(\"assistant\", f\"Here are reflections on the error: {reflections}\")]\n    return {\"generation\": code_solution, \"messages\": messages, \"iterations\": iterations}\n\n\n### Edges\n\n\ndef decide_to_finish(state: GraphState):\n    \"\"\"\n    Determines whether to finish.\n\n    Args:\n        state (dict): The current graph state\n\n    Returns:\n        str: Next node to call\n    \"\"\"\n    error = state[\"error\"]\n    iterations = state[\"iterations\"]\n\n    if error == \"no\" or iterations == max_iterations:\n        print(\"---DECISION: FINISH---\")\n        return \"end\"\n    else:\n        print(\"---DECISION: RE-TRY SOLUTION---\")\n        if flag == \"reflect\":\n            return \"reflect\"\n        else:\n            return \"generate\"","metadata":{},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"from langgraph.graph import END, StateGraph\n\nworkflow = StateGraph(GraphState)\n\n# Define the nodes\nworkflow.add_node(\"generate\", generate)  # generation solution\nworkflow.add_node(\"check_code\", code_check)  # check code\nworkflow.add_node(\"reflect\", reflect)  # reflect\n\n# Build graph\nworkflow.set_entry_point(\"generate\")\nworkflow.add_edge(\"generate\", \"check_code\")\nworkflow.add_conditional_edges(\n    \"check_code\",\n    decide_to_finish,\n    {\n        \"end\": END,\n        \"reflect\": \"reflect\",\n        \"generate\": \"generate\",\n    },\n)\nworkflow.add_edge(\"reflect\", \"generate\")\napp = workflow.compile()","metadata":{},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"question = \"How can I directly pass a string to a runnable and use it to construct the input needed for my prompt?\"\napp.invoke({\"messages\": [(\"user\", question)], \"iterations\": 0})","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Eval","metadata":{}},{"cell_type":"markdown","source":"[Here](https://smith.langchain.com/public/326674a6-62bd-462d-88ae-eea49d503f9d/d) is a public dataset of LCEL questions. \n\nI saved this as `test-code-gen`.\n\nYou can also find the csv [here](https://github.com/langchain-ai/lcel-teacher/blob/main/eval/eval.csv).","metadata":{}},{"cell_type":"code","source":"import langsmith\n\nclient = langsmith.Client()","metadata":{},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"# Clone the dataset to your tenant to use it\npublic_dataset = (\n    \"https://smith.langchain.com/public/326674a6-62bd-462d-88ae-eea49d503f9d/d\"\n)\nclient.clone_public_dataset(public_dataset)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Custom evals.","metadata":{}},{"cell_type":"code","source":"from langsmith.schemas import Example, Run\n\n\ndef check_import(run: Run, example: Example) -> dict:\n    imports = run.outputs.get(\"imports\")\n    try:\n        exec(imports)\n        return {\"key\": \"import_check\", \"score\": 1}\n    except:\n        return {\"key\": \"import_check\", \"score\": 0}\n\n\ndef check_execution(run: Run, example: Example) -> dict:\n    imports = run.outputs.get(\"imports\")\n    code = run.outputs.get(\"code\")\n    try:\n        exec(imports + \"\\n\" + code)\n        return {\"key\": \"code_execution_check\", \"score\": 1}\n    except:\n        return {\"key\": \"code_execution_check\", \"score\": 0}","metadata":{},"execution_count":8,"outputs":[]},{"cell_type":"markdown","source":"Compare LangGraph to Context Stuffing.","metadata":{}},{"cell_type":"code","source":"def predict_base_case(example: dict):\n    \"\"\"Context stuffing\"\"\"\n    solution = code_gen_chain.invoke(\n        {\"context\": concatenated_content, \"messages\": [(\"user\", example[\"question\"])]}\n    )\n    solution_structured = structured_code_formatter.invoke([(\"code\", solution)])\n    return {\"imports\": solution_structured.imports, \"code\": solution_structured.code}\n\n\ndef predict_langgraph(example: dict):\n    \"\"\"LangGraph\"\"\"\n    graph = app.invoke({\"messages\": [(\"user\", example[\"question\"])], \"iterations\": 0})\n    solution = graph[\"generation\"]\n    return {\"imports\": solution.imports, \"code\": solution.code}","metadata":{},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"from langsmith.evaluation import evaluate\n\n# Evaluator\ncode_evalulator = [check_import, check_execution]\n\n# Dataset\ndataset_name = \"test-code-gen\"","metadata":{},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"# Run base case\nexperiment_results_ = evaluate(\n    predict_base_case,\n    data=dataset_name,\n    evaluators=code_evalulator,\n    experiment_prefix=f\"test-without-langgraph-{expt_llm}\",\n    max_concurrency=2,\n    metadata={\n        \"llm\": expt_llm,\n    },\n)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Run with langgraph\nexperiment_results = evaluate(\n    predict_langgraph,\n    data=dataset_name,\n    evaluators=code_evalulator,\n    experiment_prefix=f\"test-with-langgraph-{expt_llm}-{flag}\",\n    max_concurrency=2,\n    metadata={\n        \"llm\": expt_llm,\n        \"feedback\": flag,\n    },\n)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"`Results:`\n\n* `LangGraph outperforms base case`: adding re-try loop improve performance\n* `Reflection did not help`: reflection prior to re-try regression vs just passing errors directly back to the LLM\n* `GPT-4 outperforms Claude3`: Claude3 had 3 and 1 run fail due to tool-use error for Opus and Haiku, respectively\n\n","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}